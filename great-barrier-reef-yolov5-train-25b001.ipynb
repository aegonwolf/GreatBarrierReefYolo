{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### mainly modified just this great notebook \nhttps://www.kaggle.com/awsaf49/great-barrier-reef-yolov5-train\nand \nhttps://www.kaggle.com/ultralytics/yolov5\n","metadata":{}},{"cell_type":"markdown","source":"# üõ† Install Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -qU wandb\n!pip install -qU bbox-utility # check https://github.com/awsaf49/bbox for source code","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-10T16:01:41.522047Z","iopub.execute_input":"2022-02-10T16:01:41.522599Z","iopub.status.idle":"2022-02-10T16:02:02.001966Z","shell.execute_reply.started":"2022-02-10T16:01:41.52252Z","shell.execute_reply":"2022-02-10T16:02:02.001131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìö Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\n\nimport shutil\nimport sys\nsys.path.append('../input/tensorflow-great-barrier-reef')\n\nfrom joblib import Parallel, delayed\n\nfrom IPython.display import display","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-10T16:02:02.004409Z","iopub.execute_input":"2022-02-10T16:02:02.004986Z","iopub.status.idle":"2022-02-10T16:02:02.40156Z","shell.execute_reply.started":"2022-02-10T16:02:02.004942Z","shell.execute_reply":"2022-02-10T16:02:02.40084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_key\")\n    wandb.login(key=api_key)\n    anonymous = None\nexcept:\n    wandb.login(anonymous='must')\n    print('To use your W&B account,\\nGo to Add-ons -> Secrets and provide your W&B access token. Use the Label name as WANDB. \\nGet your W&B access token from here: https://wandb.ai/authorize')","metadata":{"execution":{"iopub.status.busy":"2022-02-10T16:02:02.402765Z","iopub.execute_input":"2022-02-10T16:02:02.403027Z","iopub.status.idle":"2022-02-10T16:02:04.265605Z","shell.execute_reply.started":"2022-02-10T16:02:02.402992Z","shell.execute_reply":"2022-02-10T16:02:04.264817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìñ Meta Data\n* `train_images/` - Folder containing training set photos of the form `video_{video_id}/{video_frame}.jpg`.\n\n* `[train/test].csv` - Metadata for the images. As with other test files, most of the test metadata data is only available to your notebook upon submission. Just the first few rows available for download.\n\n* `video_id` - ID number of the video the image was part of. The video ids are not meaningfully ordered.\n* `video_frame` - The frame number of the image within the video. Expect to see occasional gaps in the frame number from when the diver surfaced.\n* `sequence` - ID of a gap-free subset of a given video. The sequence ids are not meaningfully ordered.\n* `sequence_frame` - The frame number within a given sequence.\n* `image_id` - ID code for the image, in the format `{video_id}-{video_frame}`\n* `annotations` - The bounding boxes of any starfish detections in a string format that can be evaluated directly with Python. Does not use the same format as the predictions you will submit. Not available in test.csv. A bounding box is described by the pixel coordinate `(x_min, y_min)` of its lower left corner within the image together with its `width` and `height` in pixels --> (COCO format).","metadata":{}},{"cell_type":"code","source":"FOLD      = 1 # which fold to train\nDIM       = 3600 \nMODEL     = 'yolov5s6'\nBATCH     = 4\nEPOCHS    = 10\nOPTMIZER  = 'Adam'\n\nPROJECT   = 'great-barrier-reef-public' # w&b in yolov5\nNAME      = f'{MODEL}-dim{DIM}-fold{FOLD}' # w&b for yolov5\n\nREMOVE_NOBBOX = True # remove images with no bbox\nROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'\nIMAGE_DIR = '/kaggle/images' # directory to save images\nLABEL_DIR = '/kaggle/labels' # directory to save labels","metadata":{"execution":{"iopub.status.busy":"2022-02-10T16:02:04.269429Z","iopub.execute_input":"2022-02-10T16:02:04.269976Z","iopub.status.idle":"2022-02-10T16:02:04.276377Z","shell.execute_reply.started":"2022-02-10T16:02:04.269941Z","shell.execute_reply":"2022-02-10T16:02:04.274699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Directories","metadata":{}},{"cell_type":"code","source":"!mkdir -p {IMAGE_DIR}\n!mkdir -p {LABEL_DIR}","metadata":{"execution":{"iopub.status.busy":"2022-02-10T16:02:04.279627Z","iopub.execute_input":"2022-02-10T16:02:04.279827Z","iopub.status.idle":"2022-02-10T16:02:05.599041Z","shell.execute_reply.started":"2022-02-10T16:02:04.279803Z","shell.execute_reply":"2022-02-10T16:02:05.597933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get Paths","metadata":{}},{"cell_type":"code","source":"# Train Data\ndf = pd.read_csv(f'{ROOT_DIR}/train.csv')\ndf['old_image_path'] = f'{ROOT_DIR}/train_images/video_'+df.video_id.astype(str)+'/'+df.video_frame.astype(str)+'.jpg'\ndf['image_path']  = f'{IMAGE_DIR}/'+df.image_id+'.jpg'\ndf['label_path']  = f'{LABEL_DIR}/'+df.image_id+'.txt'\ndf['annotations'] = df['annotations'].progress_apply(eval)\ndisplay(df.head(2))","metadata":{"execution":{"iopub.status.busy":"2022-02-10T16:02:05.600547Z","iopub.execute_input":"2022-02-10T16:02:05.600821Z","iopub.status.idle":"2022-02-10T16:02:06.042329Z","shell.execute_reply.started":"2022-02-10T16:02:05.60078Z","shell.execute_reply":"2022-02-10T16:02:06.041432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of BBoxes\n> Nearly 80% images are without any bbox.","metadata":{}},{"cell_type":"code","source":"df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndata = (df.num_bbox>0).value_counts(normalize=True)*100\nprint(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2022-02-10T16:02:06.043832Z","iopub.execute_input":"2022-02-10T16:02:06.044101Z","iopub.status.idle":"2022-02-10T16:02:06.139945Z","shell.execute_reply.started":"2022-02-10T16:02:06.044067Z","shell.execute_reply":"2022-02-10T16:02:06.13812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üßπ Clean Data\n* In this notebook, we use only **bboxed-images** (`~5k`). We can use all `~23K` images for train but most of them don't have any labels. So it would be easier to carry out experiments using only **bboxed images**.","metadata":{}},{"cell_type":"code","source":"if REMOVE_NOBBOX:\n    df = df.query(\"num_bbox>0\")","metadata":{"execution":{"iopub.status.busy":"2022-02-10T16:02:06.141191Z","iopub.execute_input":"2022-02-10T16:02:06.141577Z","iopub.status.idle":"2022-02-10T16:02:06.161121Z","shell.execute_reply.started":"2022-02-10T16:02:06.141543Z","shell.execute_reply":"2022-02-10T16:02:06.1604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ‚úèÔ∏è Write Images\n* We need to copy the Images to Current Directory(`/kaggle/working`) as `/kaggle/input` doesn't have **write access** which is needed for **YOLOv5**.\n* We can make this process faster using **Joblib** which uses **Parallel** computing.","metadata":{}},{"cell_type":"code","source":"def make_copy(row):\n    shutil.copyfile(row.old_image_path, row.image_path)\n    return","metadata":{"execution":{"iopub.status.busy":"2022-02-10T16:02:06.16237Z","iopub.execute_input":"2022-02-10T16:02:06.162766Z","iopub.status.idle":"2022-02-10T16:02:06.166796Z","shell.execute_reply.started":"2022-02-10T16:02:06.162731Z","shell.execute_reply":"2022-02-10T16:02:06.166136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_paths = df.old_image_path.tolist()\n_ = Parallel(n_jobs=-1, backend='threading')(delayed(make_copy)(row) for _, row in tqdm(df.iterrows(), total=len(df)))","metadata":{"execution":{"iopub.status.busy":"2022-02-10T16:02:06.170421Z","iopub.execute_input":"2022-02-10T16:02:06.17088Z","iopub.status.idle":"2022-02-10T16:02:39.298664Z","shell.execute_reply.started":"2022-02-10T16:02:06.170829Z","shell.execute_reply":"2022-02-10T16:02:39.297595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üî® Helper","metadata":{}},{"cell_type":"code","source":"# check https://github.com/awsaf49/bbox for source code of following utility functions\nfrom bbox.utils import coco2yolo, coco2voc, voc2yolo\nfrom bbox.utils import draw_bboxes, load_image\nfrom bbox.utils import clip_bbox, str2annot, annot2str\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path'])\n    return row\n\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-02-10T16:02:39.300479Z","iopub.execute_input":"2022-02-10T16:02:39.302769Z","iopub.status.idle":"2022-02-10T16:02:40.119617Z","shell.execute_reply.started":"2022-02-10T16:02:39.302714Z","shell.execute_reply":"2022-02-10T16:02:40.118905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create BBox","metadata":{}},{"cell_type":"code","source":"df['bboxes'] = df.annotations.progress_apply(get_bbox)\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T16:02:40.12268Z","iopub.execute_input":"2022-02-10T16:02:40.122908Z","iopub.status.idle":"2022-02-10T16:02:40.190919Z","shell.execute_reply.started":"2022-02-10T16:02:40.122881Z","shell.execute_reply":"2022-02-10T16:02:40.19014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get Image-Size\n> All Images have same dimension, [Width, Height] =  `[1280, 720]`","metadata":{}},{"cell_type":"code","source":"df['width']  = 1280\ndf['height'] = 720\ndisplay(df.head(2))","metadata":{"execution":{"iopub.status.busy":"2022-02-10T16:02:40.192319Z","iopub.execute_input":"2022-02-10T16:02:40.192569Z","iopub.status.idle":"2022-02-10T16:02:40.212784Z","shell.execute_reply.started":"2022-02-10T16:02:40.192534Z","shell.execute_reply":"2022-02-10T16:02:40.212168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üè∑Ô∏è Create Labels\nWe need to export our labels to **YOLO** format, with one `*.txt` file per image (if no objects in image, no `*.txt` file is required). The *.txt file specifications are:\n\n* One row per object\n* Each row is class `[x_center, y_center, width, height]` format.\n* Box coordinates must be in **normalized** `xywh` format (from `0 - 1`). If your boxes are in pixels, divide `x_center` and `width` by `image width`, and `y_center` and `height` by `image height`.\n* Class numbers are **zero-indexed** (start from `0`).\n\n> Competition bbox format is **COCO** hence `[x_min, y_min, width, height]`. So, we need to convert form **COCO** to **YOLO** format.\n","metadata":{}},{"cell_type":"code","source":"cnt = 0\nall_bboxes = []\nbboxes_info = []\nfor row_idx in tqdm(range(df.shape[0])):\n    row = df.iloc[row_idx]\n    image_height = row.height\n    image_width  = row.width\n    bboxes_coco  = np.array(row.bboxes).astype(np.float32).copy()\n    num_bbox     = len(bboxes_coco)\n    names        = ['cots']*num_bbox\n    labels       = np.array([0]*num_bbox)[..., None].astype(str)\n    ## Create Annotation(YOLO)\n    with open(row.label_path, 'w') as f:\n        if num_bbox<1:\n            annot = ''\n            f.write(annot)\n            cnt+=1\n            continue\n        bboxes_voc  = coco2voc(bboxes_coco, image_height, image_width)\n        bboxes_voc  = clip_bbox(bboxes_voc, image_height, image_width)\n        bboxes_yolo = voc2yolo(bboxes_voc, image_height, image_width).astype(str)\n        all_bboxes.extend(bboxes_yolo.astype(float))\n        bboxes_info.extend([[row.image_id, row.video_id, row.sequence]]*len(bboxes_yolo))\n        annots = np.concatenate([labels, bboxes_yolo], axis=1)\n        string = annot2str(annots)\n        f.write(string)\nprint('Missing:',cnt)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-02-10T16:02:40.214063Z","iopub.execute_input":"2022-02-10T16:02:40.214507Z","iopub.status.idle":"2022-02-10T16:02:44.592786Z","shell.execute_reply.started":"2022-02-10T16:02:40.214472Z","shell.execute_reply":"2022-02-10T16:02:44.591931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìÅ Create Folds\n> Number of samples aren't same in each fold which can create large variance in **Cross-Validation**.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\nkf = GroupKFold(n_splits = 3)\ndf = df.reset_index(drop=True)\ndf['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df, groups=df.video_id.tolist())):\n    df.loc[val_idx, 'fold'] = fold\ndisplay(df.fold.value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-02-10T16:02:44.594173Z","iopub.execute_input":"2022-02-10T16:02:44.594952Z","iopub.status.idle":"2022-02-10T16:02:45.461436Z","shell.execute_reply.started":"2022-02-10T16:02:44.594912Z","shell.execute_reply":"2022-02-10T16:02:45.460738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ‚≠ï BBox Distribution","metadata":{}},{"cell_type":"code","source":"bbox_df = pd.DataFrame(np.concatenate([bboxes_info, all_bboxes], axis=1),\n             columns=['image_id','video_id','sequence',\n                     'xmid','ymid','w','h'])\nbbox_df[['xmid','ymid','w','h']] = bbox_df[['xmid','ymid','w','h']].astype(float)\nbbox_df['area'] = bbox_df.w * bbox_df.h * 1280 * 720\nbbox_df = bbox_df.merge(df[['image_id','fold']], on='image_id', how='left')\nbbox_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T16:02:45.462713Z","iopub.execute_input":"2022-02-10T16:02:45.462997Z","iopub.status.idle":"2022-02-10T16:02:45.609476Z","shell.execute_reply.started":"2022-02-10T16:02:45.462962Z","shell.execute_reply":"2022-02-10T16:02:45.608696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## `x_center` Vs `y_center`","metadata":{}},{"cell_type":"code","source":"from scipy.stats import gaussian_kde\n\nall_bboxes = np.array(all_bboxes)\n\nx_val = all_bboxes[...,0]\ny_val = all_bboxes[...,1]\n\n# Calculate the point density\nxy = np.vstack([x_val,y_val])\nz = gaussian_kde(xy)(xy)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-10T16:02:45.610881Z","iopub.execute_input":"2022-02-10T16:02:45.611129Z","iopub.status.idle":"2022-02-10T16:02:48.304319Z","shell.execute_reply.started":"2022-02-10T16:02:45.611094Z","shell.execute_reply":"2022-02-10T16:02:48.302206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## `width` Vs `height`","metadata":{}},{"cell_type":"code","source":"x_val = all_bboxes[...,2]\ny_val = all_bboxes[...,3]\n\n# Calculate the point density\nxy = np.vstack([x_val,y_val])\nz = gaussian_kde(xy)(xy)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-10T16:02:48.30531Z","iopub.execute_input":"2022-02-10T16:02:48.305607Z","iopub.status.idle":"2022-02-10T16:02:50.79466Z","shell.execute_reply.started":"2022-02-10T16:02:48.305564Z","shell.execute_reply":"2022-02-10T16:02:50.793915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üçö Dataset","metadata":{}},{"cell_type":"code","source":"train_files = []\nval_files   = []\ntrain_df = df.query(\"fold!=@FOLD\")\nvalid_df = df.query(\"fold==@FOLD\")\ntrain_files += list(train_df.image_path.unique())\nval_files += list(valid_df.image_path.unique())\nlen(train_files), len(val_files)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T16:02:50.796224Z","iopub.execute_input":"2022-02-10T16:02:50.796748Z","iopub.status.idle":"2022-02-10T16:02:51.234473Z","shell.execute_reply.started":"2022-02-10T16:02:50.796706Z","shell.execute_reply":"2022-02-10T16:02:51.233741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ‚öôÔ∏è Configuration\nThe dataset config file requires\n1. The dataset root directory path and relative paths to `train / val / test` image directories (or *.txt files with image paths)\n2. The number of classes `nc` and \n3. A list of class `names`:`['cots']`","metadata":{}},{"cell_type":"code","source":"import yaml\n\ncwd = '/kaggle/working/'\n\nwith open(os.path.join( cwd , 'train.txt'), 'w') as f:\n    for path in train_df.image_path.tolist():\n        f.write(path+'\\n')\n            \nwith open(os.path.join(cwd , 'val.txt'), 'w') as f:\n    for path in valid_df.image_path.tolist():\n        f.write(path+'\\n')\n\ndata = dict(\n    path  = '/kaggle/working',\n    train =  os.path.join( cwd , 'train.txt') ,\n    val   =  os.path.join( cwd , 'val.txt' ),\n    nc    = 1,\n    names = ['cots'],\n    )\n\nwith open(os.path.join( cwd , 'gbr.yaml'), 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\nf = open(os.path.join( cwd , 'gbr.yaml'), 'r')\nprint('\\nyaml:')\nprint(f.read())","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-10T16:02:51.236117Z","iopub.execute_input":"2022-02-10T16:02:51.236426Z","iopub.status.idle":"2022-02-10T16:02:51.252303Z","shell.execute_reply.started":"2022-02-10T16:02:51.23638Z","shell.execute_reply":"2022-02-10T16:02:51.251402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/hyp.yaml\nlr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\nlrf: 0.01  # final OneCycleLR learning rate (lr0 * lrf)\nmomentum: 0.97  # SGD momentum/Adam beta1\nweight_decay: 0.0005  # optimizer weight decay 5e-4\nwarmup_epochs: 2.0  # warmup epochs (fractions ok)\nwarmup_momentum: 0.9  # warmup initial momentum\nwarmup_bias_lr: 0.1  # warmup initial bias lr\nbox: 0.05  # box loss gain\ncls: 0.5  # cls loss gain\ncls_pw: 1.0  # cls BCELoss positive_weight\nobj: 1.0  # obj loss gain (scale with pixels)\nobj_pw: 1.0  # obj BCELoss positive_weight\niou_t: 0.30  # IoU training threshold\nanchor_t: 4.0  # anchor-multiple threshold\n# anchors: 3  # anchors per output layer (0 to ignore)\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\nhsv_h: 0.05  # image HSV-Hue augmentation (fraction)\nhsv_s: 0.5  # image HSV-Saturation augmentation (fraction)\nhsv_v: 0.3  # image HSV-Value augmentation (fraction)\ndegrees: 45.0  # image rotation (+/- deg)\ntranslate: 0.10  # image translation (+/- fraction)\nscale: 0.5  # image scale (+/- gain)\nshear: 0.0  # image shear (+/- deg)\nperspective: 0.0  # image perspective (+/- fraction), range 0-0.001\nflipud: 0.5  # image flip up-down (probability)\nfliplr: 0.5  # image flip left-right (probability)\nmosaic: 0.5  # image mosaic (probability)\nmixup: 0.5 # image mixup (probability)\ncopy_paste: 0.0  # segment copy-paste (probability)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-10T16:02:51.25398Z","iopub.execute_input":"2022-02-10T16:02:51.254511Z","iopub.status.idle":"2022-02-10T16:02:51.262895Z","shell.execute_reply.started":"2022-02-10T16:02:51.254475Z","shell.execute_reply":"2022-02-10T16:02:51.261821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working\n!rm -r /kaggle/working/yolov5\n# !git clone https://github.com/ultralytics/yolov5 # clone\n!cp -r /kaggle/input/yolov5-lib-ds /kaggle/working/yolov5\n%cd yolov5\n%pip install -qr requirements.txt  # install\n\nfrom yolov5 import utils\ndisplay = utils.notebook_init()  # check","metadata":{"execution":{"iopub.status.busy":"2022-02-10T16:02:51.264331Z","iopub.execute_input":"2022-02-10T16:02:51.264944Z","iopub.status.idle":"2022-02-10T16:03:03.367766Z","shell.execute_reply.started":"2022-02-10T16:02:51.264907Z","shell.execute_reply":"2022-02-10T16:03:03.366945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üöÖ Training","metadata":{}},{"cell_type":"code","source":"!python train.py --img {DIM}\\\n--batch {BATCH}\\\n--epochs {EPOCHS}\\\n--optimizer {OPTMIZER}\\\n--data /kaggle/working/gbr.yaml\\\n--hyp /kaggle/working/hyp.yaml\\\n--weights {MODEL}.pt\\\n--project {PROJECT} --name {NAME}\\\n--exist-ok","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-10T16:03:03.369599Z","iopub.execute_input":"2022-02-10T16:03:03.37048Z","iopub.status.idle":"2022-02-10T16:03:58.132333Z","shell.execute_reply.started":"2022-02-10T16:03:03.370433Z","shell.execute_reply":"2022-02-10T16:03:58.131253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Output Files","metadata":{}},{"cell_type":"code","source":"OUTPUT_DIR = '{}/{}'.format(PROJECT, NAME)\n!ls {OUTPUT_DIR}","metadata":{"execution":{"iopub.status.busy":"2022-02-10T16:03:58.13354Z","iopub.status.idle":"2022-02-10T16:03:58.134414Z","shell.execute_reply.started":"2022-02-10T16:03:58.13415Z","shell.execute_reply":"2022-02-10T16:03:58.134179Z"},"trusted":true},"execution_count":null,"outputs":[]}]}